---
title: HA - 集群
category: 数据库
tag:
 - Redis
---





## 概述

Redis提供了去中心化的**集群部署**模式，集群内所有Redis节点之间两两连接，而很多的客户端工具会根据key将请求分发到对应的分片下的某一个节点上进行处理。

一个典型的Redis集群部署场景如下图所示：

![](https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406122359313.webp)

在Redis集群里面，又会划分出`分区`的概念，一个集群中可有多个分区。分区有几个特点：

1. 同一个分区内的Redis节点之间的数据完全一样，多个节点保证了数据有多份副本冗余保存，且可以提供高可用保障。
2. 不同分片之间的数据不相同。
3. 通过水平增加多个分片的方式，可以实现整体集群的容量的扩展。

按照Cluster模式进行部署的时候，要求最少需要部署`6个`Redis节点（3个分片，每个分片中1主1从），其中集群中每个分片的master节点负责对外提供读写操作，slave节点则作为故障转移使用（master出现故障的时候充当新的master）、对外提供只读请求处理。



## 集群数据分布策略

### Redis Sharding（数据分片）

在`Redis Cluster`前，为了解决数据分发到各个分区的问题，普遍采用的是`Redis Sharding`（数据分片）方案。所谓的Sharding，其实就是一种数据分发的策略。根据key的hash值进行取模，确定最终归属的节点。

使用Redis Sharding方式进行数据分片的时候，当集群内数据分区个数出现变化的时候，比如集群扩容的时候，会导致请求被分发到错误节点上，导致缓存*命中率降低*。

![img](https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406130001852.webp)

如果需要解决这个问题，就需要对原先扩容前已经存储的数据重新进行一次hash计算和取模操作，将全部的数据重新分发到新的正确节点上进行存储。这个操作被称为`重新Sharding`，重新sharding期间服务不可用，可能会对业务造成影响。



### 一致性Hash

为了降低节点的增加或者移除对于整体已有缓存数据访问的影响，最大限度的保证缓存命中率，改良后的[`一致性Hash`](https://www.seven97.top/microservices/protocol/consistencyhash.html)算法浮出水面。



### Hash槽

不管是原本的Redis Sharding，还是经过改良后的一致性Hash，在节点的新增或者删减的时候，始终都会出现部分缓存数据丢失的问题 —— 只是丢失的数据量的多少区别。如何才能实现扩展或者收缩节点的时候，保持已有数据不丢失呢？

既然动态变更调整的方式行不通，那就手动指定咯！Hash槽的实现策略因此产生。何为Hash槽？Hash槽的原理与HashMap有点相似，共有16384个槽位，每个槽位对应一个数据桶，然后每个Redis的分区都可以负责这些hash槽中的部分区间。存储数据的时候，数据key经过Hash计算后会匹配到一个对应的槽位，然后数据存储在该槽位对应的分片中。然后各个分区节点会与Hash槽之间有个映射绑定关系，由指定的Redis分区节点负责存储对应的Has槽对应的具体分片文件。

![](https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406130005738.webp)

数据查询的时候，先根据key的Hash值进行计算，确定应该落入哪个Hash槽，进而根据映射关系，确定负责此Hash槽数据存储的redis分区节点是哪个，然后就可以去做对应的查询操作。

执行数据节点增加的时候，需要手动执行下处理：

- 为新的节点分配新其负责的Hash槽位区间段；
- 调整已有的节点的Hash槽位负责区间段；
- 将调整到新节点上的hash槽位区间段对应的数据分片文件拷贝到新的节点上。

这样，就不会出现已有数据无法使用的情况了。鉴于Hash槽的自主可控性以及节点伸缩场景下的优势，其也成为了Redis Cluster中使用的方案。









